\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{amsmath,tkz-euclide}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, decorations.markings}
\usetkzobj{all}
\counterwithin{figure}{section}
\graphicspath{{/Users/edwardmcdugald/Research/convection_patterns_wip/figs/writeup/}}
\numberwithin{equation}{section}

\parindent 0in
\parskip 1em
\Urlmuskip=0mu plus 1mu

\title{An open problem in pattern forming systems}
\author{Edward McDugald}


\begin{document}

\maketitle
\section{Introduction}
\par The aim of this report is to describe an open problem in pattern forming systems, pertaining specifically to patterns deriving from microscopic gradient flows that are translationally and rotationally invariant, and that admit straight parallel roll solutions on infinite domains. On finite domains however, such rolls may emerge in different locations with different orientations, and they meet in the interior of the domain generating point and line defects. Such pattern forming systems, while arising from various microscopic gradient systems, obey a universal PDE describing the evolution of the phase variable, $\theta$. 
\par A canonical example of such patterns in nature is seen through Rayleigh-B\'{e}nard convection. Rayleigh-B\'{e}nard convection may be observed by trapping a thin layer of fluid between two plates, and applying heat to the bottom plate. In the microscopic model which we will consider, the stress parameter $R$ is the difference in heat between the top and bottom plate. Initially heat will transfer through the fluid by conduction, however past a critical value, heat will transfer through convection, generating a velocity field that manifests as temperature rolls in the fluid. In the figures we will present in this report, we may consider the surfaces as being the average temperature along the $z$ direction. 
\begin{figure}
\centering
\includegraphics[scale=0.5]{convection_roll_examples.png}
\caption{Numerical and experimental depictions of convection rolls.}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.4]{cr_phase.png}
\caption{Local Periodicity of convection rolls.}
\end{figure}


\par The procedure for deriving such a phase equation involves studying the vector field whose components are the instantaneous frequencies of the pattern generated by the microscopic equations. We call this vector field $\bm{k}$, and note that this is the gradient of the phase, $\bm{k} = \nabla \theta$. The phase gradient is modulated over distances $L$, the average distance between defects, which is long with respect to the local wavelength $l$. We introduce the following variables:
\begin{align*}
    \bm{X} &= \epsilon\bm{x}\\
    T &= \epsilon^2 t\\
    \Theta(\bm{X},T) &= \epsilon \theta(\bm{x},t)\\
    k &= \|\bm{k}\|\\
    \epsilon &= \frac{l}{L} \quad \text{The inverse aspect ratio}.
\end{align*}
Letting $w$ be the variable of the microscopic equations, and $E$ the associated energy functional, we apply the following modulational Ansatz:
\begin{align}
    w &= \sum A_n(k^2)\cos(n\theta) \quad \text{(for real fields)}\\
    w &= A(k^2,\bm{X},T)e^{i\theta(\bm{x},t)} \quad \text{(for complex fields)}.
\end{align}
We then average the energy over the peridicity of the pattern
\begin{equation}
    \overline{E} = \frac{1}{2\pi} \int E d\theta
\end{equation}
and analyze the dependence of the energy on $A$ and $\Theta$ in orders of $\epsilon^n$. We find that the only orders of $\epsilon$ that contribute are $\mathcal{O}(\epsilon^0)$ and $\mathcal{O}(\epsilon^2)$, and that the energy can be interpreted as a sum of stretching and bending energies. We also find that stationary solutions can be deduced by balancing the stress and strain energies. We pose an open challenge, where a certain pattern appearing in simulations of microscopic gradient flows, and in experiments, is not yet deduced by the Cross-Newell equation. We also discuss some potential uses of machine learning to approach this problem. 
\section{The Regularized Cross-Newell Phase Diffusion Equation for Complex Fields}
\subsection{Deriving the Unegularized Equation}
\par We consider pattern forming systems that arise as PDEs that are equivalently gradient flows. These are a very general object, and we take as our preferred example to be the Swift Hohenberg equation. In its complexified form, the swift hohenberg equation has energy density
\begin{equation}
    E = \int \left( (\nabla^2+1)w(\nabla^2+1)w^{*}-Rww^{*}+\frac{1}{2}w^2w^{*^2}\right)dx.
\end{equation}
Let us compute $\delta E$,
\begin{align*}
    \delta E &= \lim_{h \rightarrow 0}\frac{E[w+h\delta w, w^* + h\delta w^*]-E[w,w^*]}{h}\\
        &= \begin{aligned}[t]
            & \int \left( (\nabla^2+1)w(\nabla^2+1)\delta w^* -Rw\delta w^* + w^2w^*\delta w^* + \right. \\
            & \left. (\nabla^2+1)\delta w (\nabla^2+1)w^* -Rw^*\delta w + ww^{*^2}\delta w \right) d\bm{x}    
        \end{aligned}
\end{align*}
Using integration by parts, one finds that
\begin{align*}
    (\nabla^2+1)w(\nabla^2+1)\delta w^* &= (\nabla^2 +1)^2w\delta w^*\\
    (\nabla^2+1)\delta w (\nabla^2+1)^2w^* &= (\nabla^2+1)^2w^*\delta w.
\end{align*}
Thus, one obtains
\begin{align}
    \frac{\delta E}{\delta w^*} &= (\nabla^2+1)^2w - Rw + w^2w^*\\
    \frac{\delta E}{\delta w} &= (\nabla^2+1)^2 w^* -Rw^* + w^{*^2}w.
\end{align} 
The complexified Swift-Hohenberg equations can be written as
\begin{equation}
    w_t = - \frac{\delta E}{\delta w^*}, \quad w^*_2 = -\frac{\delta E}{\delta w}.
\end{equation}
Using the Ansatz for complex fields (1.2), one can write
\begin{equation}
    \delta E = \frac{\delta E}{\delta w}\delta w + \frac{\delta E}{\delta w^*}\delta w^* = -2A_t\delta A - 2A^2\Theta_t \delta \theta.
\end{equation}
\par We want now to find expressions for $\frac{\delta E}{\delta A}$ and $\frac{\delta E}{\delta \Theta}$
Using the fact that $\nabla_{\bm{x}} = \epsilon\nabla_{\bm{X}}$, and writing $w=Ae^{i\theta}=A(\bm{x})e^{i\theta(\bm{x})}$, one obtains
\begin{align}
    \nabla_{\bm{x}}w &= e^{i\theta}(i\bm{k}+\epsilon\nabla_{\bm{X}})A\\
    \nabla^2_{\bm{x}}w &= e^{i\theta}(-k^2+i\epsilon(2\bm{x}\cdot \nabla_{\bm{X}}+\nabla_{\bm{X}}\cdot\bm{k})+\epsilon^2\nabla^2_{\bm{X}})A\\
 \nabla_{\bm{x}}w^* &= e^{-i\theta}(-i\bm{k}+\epsilon\nabla_{\bm{X}})A\\
    \nabla^2_{\bm{x}}w^* &= e^{-i\theta}(-k^2-i\epsilon(2\bm{x}\cdot \nabla_{\bm{X}}+\nabla_{\bm{X}}\cdot\bm{k})+\epsilon^2\nabla^2_{\bm{X}})A.
\end{align}
Inserting (1.2) into (2.1) with (2.6)-(2.9), and \emph{ignoring} terms of order $\mathcal{O}(\epsilon^3)$, one obtains
\begin{align}
    E &= \begin{aligned}[t]
         &\int \left( (k^2-1)^2A^2 - RA^2 + \frac{1}{4}A^4 + K\right)d\bm{x}\\
         & + \epsilon^2 \int \left( (2\bm{k}\cdot \nabla A + \nabla \cdot \bm{k}A)^2 + 2(1-k^2)A\nabla^2A \right)d\bm{x}.
         \end{aligned}
\end{align}
Now, obtaining macroscopic equations in terms of the phase parameter requires averaging the energy over the periodicity of the pattern. Conveniently, in the complex field case, this averaging occurs automatically, and we have $E = \overline{E}$. Equation (2.5) gives
\begin{equation}
    -2A_t = \frac{\delta \overline{E}}{\delta A} = \frac{\delta\overline{E_0}}{\delta A}+\epsilon^2 \frac{\delta \overline{E_2}}{\delta A}.
\end{equation}
By construction, $A_t = \epsilon^2 A_T$, and therefore $A_t$ is at most $\mathcal{O}(\epsilon^2)$. Thus, to leading order, the main equation contributing to the evolution of the amplitude $A$ is 
\begin{equation}
    \frac{\delta \overline{E_0}}{\delta A} = 0.
\end{equation}
This gives
\begin{equation}
    A^2 = R - (K^2-1)^2.
\end{equation}
This is known as the eikonal solution, which demands that the amplitude be slaved algebraically to the modulus of the phase gradient. This property holds for stress parameter values far from onset. 
\par Having found a dominant relation for $A$, we seek an equation for $\Theta$. Using the eikonal solution, we can write the leading order energy as 
\begin{equation}
    \overline{E_0} = \int - \frac{1}{2}\left( A^2(k^2) \right)^2 + K.
\end{equation}
Considering just the $\mathcal{O}(\epsilon^0)$ contribution, equation (2.5) gives 
\begin{equation}
    A^2 \theta_t = -\frac{1}{2}\frac{\delta \overline{E_0}}{\delta \theta}.
\end{equation}
We have
\begin{align*}
    \frac{\delta \overline{E_0}}{\delta \theta}\delta \theta &= \delta\left( -\frac{1}{2}(A^2(k^2))^2\right)\\
                                                             &= -A^2(k^2)\frac{d}{dk^2}A^2(k^2)2\bm{k}\cdot \delta \nabla \theta.
\end{align*}
Using integration by parts, one finds that
\begin{equation*}
    -A^2(k^2)\frac{d}{dk^2}A^2(k^2)2\bm{k}\cdot \delta \nabla \theta = A^2(k^2)\frac{d}{dk^2}A^2(k^2)2\nabla \cdot \bm{k}\delta \theta.
\end{equation*}
Thus, we obtain
\begin{equation*}
    A^2 \theta_t = -A^2(k^2)\frac{dA^2(k^2)}{dk^2}2\nabla\cdot\bm{k}
\end{equation*}
Noting that $\theta_t = \epsilon \Theta_t$, $\nabla_{\bm{x}}=\epsilon\nabla_{\bm{X}}$, and setting
\begin{equation}
    B(k^2) = A^2(k^2)\frac{dA^2(k^2)}{dk^2},
\end{equation}
one obtains the \emph{unregularized} Cross-Newell phase diffusion equation
\begin{equation}
    A^2\Theta_T + \nabla_{\bm{X}}\cdot \bm{k}B(k^2) = 0.
\end{equation}
\subsection{Regularizing the Cross-Newell phase diffusion equation}
Recall the form of $\overline{E_0}$, $\overline{E_0} = \int -\frac{1}{2}A^4(k^2)+K$.
Let us define $k_B$ to be the preferred wave number, by which we mean the wave number that minimizes $E_0$.
That is, $k_B$ is defined to satisfy the equation
\begin{equation}
    \frac{d}{dk^2}A^4(k^2) = 0.
\end{equation}
We define $K$ to be $-\frac{1}{2}A^4(k^2)$ evaluated at $k_B$, and we can then write
\begin{equation}
    \overline{E_0} = \int \left( -\frac{1}{2}A^4(k^2)\right)_{k^2}^{k_B^2}d\bm{x}
\end{equation}
Note that this choice of $K$ ensures that the leading order energy is $0$ whenever $k = k_B$.
\par Let us look at the unregularized equation (2.17). We can write this as a diffusion equation in standard form
\begin{equation}
    A^2\Theta_T = -\nabla_{\bm{X}}\left[ B(k^2)\nabla \theta\right].
\end{equation}
Expanding the right hand side yields
\begin{equation}
    A^2\Theta_T = \theta_{xx}\left(-\frac{dB(k^2)}{dk^2}2\theta_x^2 - B(k^2)\right) + \theta_{xy}\left(-\frac{dB(k^2)}{dk^2}4\theta_x\theta_y\right) + \theta_{yy}\left(-\frac{dB(k^2)}{dk^2}2\theta_y^2 -B(k^2)\right).
\end{equation}
Consider the simple case where $\bm{k} = \langle \theta_x, 0\rangle$.
We then obtain
\begin{equation*}
    A^2\Theta_T = \theta_{xx}\left(-\frac{dB(k^2)}{dk^2}2\theta_x^2 - B(k^2)\right) + \theta_{yy}\left(-B(k^2)\right). 
\end{equation*}
To have a well-posed diffusion equation, we need that 
\begin{align}
    -\frac{dB(k^2)}{dk^2}2\theta_x^2 - B(k^2) &>0\\
    -B(k^2) &>0
\end{align}
Noting that 
\begin{equation}
    \frac{d}{dk}\left(kB(k^2)\right) = B(k^2)+k\frac{d}{dk}B(k^2) = B(k^2)+2k^2\frac{d}{dk^2}B(k^2),
\end{equation}
We see that we require
\begin{align}
    \frac{d}{dk}(kB(k^2)) &<0\\
    B(k^2) &< 0.
\end{align}
It turns out that $B, (kB)'$ are the eigenvalues of the second order quasi-linear operator, and for all $k$ the above must hold. Let us consider the plot of $k$ vs $kB$. For the case of Swift-Hohenberg with $R=.5$, we have $B = -4(k^2-1)(.5-(k^2-1)^2)$. We see that there is a limited range of wave numbers where these values are satisfied. This range of wave numbers is knowen as the Busse-Balloon, and is the range $k_B<k<k_E$ as labeled in figure 2.2. It turns out for natural patterns, almost everywhere $k=k_B$, however along line defects we will have $k<k_B$.  
\begin{figure}
\centering
\includegraphics[scale=0.5]{mybb.png}
\caption{Busse Balloon for Swift Hohenberg, $R=.5$}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{KB.png}
\caption{Generic Busse Balloon}
\end{figure}

Consider, for example, the scenario depicted in figure 2.3. We see that along the line defect, the pattern has a longer wavelength. This means that the local spatial frequences are smaller than they are along most of the pattern, where the wavenumber is close to $k_B$. Thus, along the line defect $k<K_B$, which lies outside of the Busse Balloon. Hence, the unregularized Cross-Newell phase diffusion equation is ill-posed along line defects, and regularization is required.
\begin{figure}
\centering
\includegraphics[scale=2.0]{wavenumber_mismatch.png}
\caption{Change in wavelength along line defects}
\end{figure}
\par To regularize the Cross-Newell phase diffusion equation, we analyze the $\mathcal{O}(\epsilon^2)$ part of the energy, $\overline{E_2}$.
We have that
\begin{equation}
    \overline{E_2} = \int \left((2\bm{k}\cdot \nabla A+\nabla \cdot \bm{k}A)^2+2(1-k^2)A\nabla^2A\right)d\bm{x}.
\end{equation}
It turns out that since $\nabla_{\bm{x}} = \bm{k}\frac{\partial}{\partial \theta}+\epsilon \nabla_{\bm{X}}$, the terms either involve differentiating $\bm{k}$ as it changes direction along the phase contour, or differentiation of $k^2$ in the direction of $\bm{k}$. The terms $2\bm{k}\cdot \nabla A$ and $A\nabla^2A$ are small, and $\overline{E_2}$ can be well approximated by
\begin{equation}
    \int A^2(k_B^2)(\nabla \cdot \bm{k})^2d\bm{x}.
\end{equation}
Computing its variation, we obtain the regularized Cross-Newell phase diffusion equation
\begin{equation}
    A^2(k_B^2)\Theta_T + \nabla \cdot \bm{k}B + \epsilon^2 A^2(k_B^2)\nabla^4\Theta = 0.
\end{equation}
Moreover, we can approximate the energy $\overline{E}$ by
\begin{equation}
\overline{E} = \int \left(\left( (\frac{d}{dk^2}B(k^2))\right)_{k_B^2}(k^2-k^2_B) + \epsilon^2 (\nabla \cdot \bm{k})^2A^2(k_B^2)\right)d\bm{x}.
\end{equation}
The $\mathcal{O}(\epsilon^0)$ contribution is the "strain" energy, and the $\mathcal{O}(\epsilon^2)$ contribution is the "bending" energy.
\section{The regularized phase diffusion equation for real fields}
In the case of real fields, the Swift-Hohenberg equation has microscopic energy functional
\begin{equation}
    E = \int \left( ((\nabla^2+1)w)^2-\frac{1}{2}Rw^2 + \frac{1}{4}w^4 \right)d\bm{x}.
\end{equation}
In this case, the microscopic equation is
\begin{equation}
    w_t = -\frac{\delta E}{\delta w} = -(\nabla^2+1)^2w + Rw - w^3.
\end{equation}
Note that equation (3.2) admits stationary solutions for striped patterns
\begin{equation}
    w(\bm{x},t) = w(\theta; \{A_n(k^2)\},R) = \sum A_n(k^2)\cos(n\theta).
\end{equation}
In the real field case, our modulation ansatz is given by (1.1),
\begin{equation}
    w(\bm{x},t) = w(\theta; \{A_n(k^2)\},R; \nabla \theta = \bm{k}(\bm{X},T)=\nabla_{\bm{X}}\Theta(\bm{X},t)).
\end{equation}
The procedure to derive the phase diffusion equation is the same as it was for the case of complex fields. However, the energy is not automatically averaged, making the procedure far more delicate. We simply state the key results.
\par 
 The microscopic equations evolve according to
 \begin{equation}
     w_t \delta w = -\delta E.
 \end{equation}
 Upon averaging over $\theta$, one obtains
 \begin{equation}
     \langle w_{\theta}^2\rangle\theta_t \delta \theta = -\delta \overline{E}
 \end{equation}
 The averaged energy to leading order is given by
\begin{equation}
    \overline{E_0} = \left(-\frac{1}{4}\langle w^4 \rangle + K \right)dxdy
\end{equation}
Again, $k^2_B$ is defined so as to satisfy
\begin{equation}
    \frac{d}{dk^2}\left(\frac{1}{4}\langle w^4 \rangle\right) = 0, 
\end{equation}
and defining $K$ as we did for the complex case, obtain
\begin{equation}
    \overline{E_0} = \int \left( \frac{1}{4}\langle w^4 \rangle\right)_{k^2}^{k^2_B}dxdy.
\end{equation}
Considering just the leading order contribution of the energy, we can obtain the unregularized diffusion equation by taking the variaton of $\overline{E_0}$, and obtain
\begin{equation}
    \langle w_{\theta}^2 \rangle \Theta_T + \nabla_{\bm{X}}\bm{k}B(k^2)=0,
\end{equation}
where 
\begin{equation}
    B(k^2) = 2\frac{dE_0}{dk^2},
\end{equation}
where $E_0$ here is used to denote the integrand of $\overline{E_0}$. Note that in the complex case, we had $B(k^2)=\frac{dE_0}{dk^2}$.
\par The fact that the $\mathcal{O}(\epsilon)$ terms vanish in the averaged energy is not as clear in the case of the real field, but it still holds. The argument as to why is omitted for the time being. 
\par Considering the $\mathcal{O}(\epsilon^2)$ terms, we argue in a similar fashion as we did for the case of complex fields. All terms involving $(\bm{k}\cdot \nabla_{\bm{X}})$ acting on the amplitudes yield terms proportional to $\frac{dA_n(k^2)}{dk^2}(\bm{k}\cdot \nabla_{\bm{X}}k^2)$ which can be written as $\bm{k}\cdot \nabla_{\bm{X}}(k^2-k_B^2)$. Since $k^2-k_B^2$ is small almost everywhere, there terms are negligible compared to the terms proportional to $\nabla \cdot \bm{k}$. Via integration by parts, one can obtain
\begin{equation}
    \epsilon^2 \int \langle w_{\theta}^2 \rangle (\nabla \cdot \bm{k})^2 d\bm{x} \approx \epsilon^2 \langle w_{\theta}^2 \rangle_{k_B}\int (\nabla \cdot \bm{k})^2 d\bm{x}. 
\end{equation}
The regularized phase diffusion equation is then
\begin{equation}
    \langle w_{\theta}^2 \rangle \Theta_T + \nabla \cdot \bm{k}B + \epsilon^2 \langle w_{\theta}^2 \rangle\nabla^4 \Theta = 0.
\end{equation}
As we did for the complex field case, we can write the phase diffusion equation as a gradient flow, with the form
\begin{align}
    \eta \nabla^4 \theta + \nabla \cdot \bm{k}B &= -\frac{\delta \overline{E}}{\delta \theta}\\
    \overline{E} &= \int \left( \frac{1}{2}\eta|\nabla^2\theta|^2 + \frac{\alpha}{2}G^2 \right)d\bm{x}\\
    G^2 &= -\frac{1}{\alpha}\int_{k_B^2}^{k^2}Bdk^2\\
    \alpha &= -\frac{1}{4k_B}|B'(k_B)|.
\end{align}
In this form, the first term measures the "bending" energy, whereas the second term measures the "stretching" energy.
\par As a quick demonstration of the fact that the wavenumber is almost constant throughout the pattern, and that the solution evolves so as to minimize an energy, figure 3.1 shows a solution of Swift-Hohenberg on the square alongside its energy density, and keep track of the total energy. We see that the energy tends to condense around bends within the pattern. We use random initial conditions for this solution.\newline
\textbf{Total energy corresponding to figure 3.1}
\begin{verbatim}
total energy at time 15:  -33.888597248321645
total energy at time 20:  -52.26404570245001
total energy at time 25:  -77.2887255255116
total energy at time 30:  -109.74938099881015
total energy at time 199:  -338.9569147336093
total energy at time 200:  -338.9892785820569
\end{verbatim}
\begin{figure}
\centering
\includegraphics[scale=0.5]{sh_sq.png}
\caption{Swift-Hohenberg solution on the square, $R=.5$}
\end{figure}
\section{Self-Dual Solutions and an Open Challenge}
\par We are interested in stationary solutions of the RCN, namely solutions of
\begin{equation}
    \eta \nabla^4 \theta + \nabla \cdot \bm{k}B = -\frac{\delta \overline{E}}{\delta \theta} = 0.
\end{equation}
Many stationary solutions can be deduced by balancing the strain and bending energies of $\overline{E}$. To name a few, the creation of VX pairs (figure 4.1), and concave and convex disclinations (figure 4.2).
\begin{figure}
\centering
\includegraphics[scale=0.5]{VXpairs.png}
\caption{The nipple instability and birth of VX pair}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{concv_convx_disc.png}
\caption{A concave and convex disclination}
\end{figure}
\par There is a particular scenario that can be observed experimentally and in numerical simulations that is not yet deduced from the RCN. Numerically, this can be depicted by solving Swift-Hohenberg on an ellipse, using the Eikonal Solution as an initial condition. What one observes qualitatively is that the eikonal solution generates pointy edges where the stripes meet along the major axis of the ellipse. These points rapidly transition into a nipple instability, at which point a new state emerges. This state appears like a set of stair steps for stripes close to the major axis. An explanation for how this state emerges, in terms of the self-dual solution, is unclear. Figure 4.3 demonstrates solutions having this behavior, with their associated energy density.\newline
\textbf{Total energy corresponding to figure 4.3}
\begin{verbatim}
total energy at time 15:  -167.19072805295662
total energy at time 20:  -261.4739829791277
total energy at time 25:  -391.7822674764031
total energy at time 30:  -560.5206723618704
total energy at time 199:  -1518.3264485682134
total energy at time 200:  -1518.3769630101526
\end{verbatim}
\begin{figure}
\centering
\includegraphics[scale=0.4]{sh_ellipse.png}
\caption{Swift Hohenberg solution on an ellipse, with eikonal initial condition}
\end{figure}
\section{Machine Learning Experiments}
\subsection{SINDy-like methods for PDEs}
\par Note that the problem at hand involves a microscopic model exhibiting behavior that is not yet deduced from the macroscopic model. This raises the possibility that the macroscopic model does not capture all aspects of the microscopic equations. Thus, one may be tempted to find an additional term to the macroscopic model. This idea motivates the study of existing methods for discovering governing equations from data.
\par Perhaps the most well known method is the Sparse Identification of Nonlinear Dynamics, or SINDy, introduced by Kutz et al for discovery of equations governing dynamical systems. Since the original publication, SINDy-like methods have been proposed to discover PDEs. As a preliminary first step, we combined ideas from Kutz and Schaefer to discover the Swift Hohenberg PDE from data consisting of solutions on the square. 
\par Using a square grid of dimension $256 \times 256$ on the interval $[-25\pi,25\pi]^2$, we integrate Swift-Hohenberg from $t=0$ to $t=100$ using time step $h = .5$. Thus, we obtain a dataset of dimension $256 \times 256 \times 201$. We used backward finite differences to compute $200$ time derivatives $u_t(x_i,y_j,t_k)$. We then form various features out of the data $u(x_i,y_j,t_k)$. These features can be any term we suspect might be in the PDE, and primarily consists of polunomial functions, trigonometric functions, and spatial derivatives of $u$. We then form the matrix equation $u_t = \Theta(u)\xi$, where $\Theta$ is a matrix whose columns are the features constructed from $u$. The task then is to solve $u_t = \Theta(u)\xi$ where $\xi$ is a sparse vector of coefficients. Figure 5.1 shows an example of the matrix equation one might form where $u$ depends on only one spatial variable.
\begin{figure}
\centering
\includegraphics[scale=0.5]{pde_sindy_lib.png}
\caption{SINDy equation for PDEs with one spatial variable}
\end{figure}
\par Once the time derivatives are obtained and feature matrix is constructed, the task is to approximate a solution of
\begin{equation}
    \xi = \text{argmin}_{\hat{\xi}}\| \Theta(u)\hat{\xi}-u_t\|_2^{2}+\lambda \|\hat{\xi}\|_{0}.
\end{equation}
In practice, what we actually solve is
\begin{equation}
    \xi = \text{argmin}_{\hat{\xi}}\| \Theta(u)\hat{\xi}-u_t\|_2^2+\lambda \|\hat{\xi}\|_{1}.
\end{equation}
Following the approach of Kutz, we use Sequential Threshold Ridge Regression with a test/train split, outlined in algorithm 1 and algorithm 2.
\begin{figure}
\centering
\includegraphics[scale=0.5]{STRidge.png}
\caption{Algorithm 1: STRidge}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{TrainSTRidge.png}
\caption{Algorithm 2: TrainSTRidge}
\end{figure}
\par Following the approach of Schaefer, we use coarsened grids to reduce the data being passed to the optimization procedure, and use spectral methods to form the spatial derivatives being passed to our library. Note that all polynomials, spatial derivatives and time derivatives are reshaped into column vectors and stacked vertically. For this preliminary proof of concept, our library is constructed with the terms $1,u,u^2,u^3,u^4,u_x, u_y, u_x^2, u_y^2, u_x u_y, u_{xy}, u_{xx}, u_{yy},\nabla^2u$, and $\nabla^4 u$. Figure 5.4 displays the code used for the coarsening procedure and the formation of $\Theta(u)$.
\begin{figure}
\centering
\includegraphics[scale=0.5]{PDEFindCode1.png}
\caption{Code for coarsening and SINDy library}
\end{figure}
\par Having formed $u_t$ and $\Theta(u)$, we call the TrainSTRidge algorithm, and recover the Swift-Hohenberg PDE, as seen in figure 5.5.
\begin{figure}
\centering
\includegraphics[scale=0.5]{PDEFindCode2.png}
\caption{Result of optimization procedure}
\end{figure}
\subsection{Combining SINDy with an Autoencoder}
\par Having "discovered" the form of the microscopic model from numerical simulations, a natural next step is to discover the macroscopic model. The use of autoencoder neural networks have been demonstrated to recover dynamics in terms of an alternative coordinate system. Consider for example, a video of a swinging pendulum. Kutz et al were able to combine an autoencoder with the SINDy framework to obtain the equation $\ddot{\theta} = \sin(\theta)$. However, we are not aware of examples where the recovered model is a PDE. The one example explored on PDE data was with a coupled reaction-diffusion system, from which the SINDy + Autoencoder method recovered a dynamical system. Thus, we are unaware of any examples of using an autoencoder with SINDy to recover a PDE.
\par The method trains a neural network to generate its own input, with a latent space (typically of reduced dimension) in the middle. Thus, we can break the autoencoder into two components, the encoder and the decoder. If $x$ is our input data, $z$ the latent space representation of $x$, and $\hat{x}$ the output, the autoencoder can be represented as
\begin{equation}
    \hat{x} = \psi(z) = \psi(\varphi(x)).
\end{equation}
In this case $\varphi$ is the encoder and $\psi$ is the decoder. The method consists of minimizing the autoencoder error, coupled with minimizing the SINDy error applied to the latent space representation. The loss function for a single example is then
\begin{equation}
    L(x) = \|x - \psi(z)\|_2^2 + \lambda_1\|(\nabla_x z)\dot{x}-\Theta(z)\xi\|_2^2 + \lambda_2\|\xi\|_1.
\end{equation}
\par Despite a lack of working examples, we set out to explore this concept, in the hopes of recovering the RCN from Swift-Hohenberg data. While one could hypothetically generate phase surfaces, and apply the standard SINDy procedure to those surfaces, it is a more interesting challenge to discover the model without asserting knowledge of the expected coordinate transformation. The first step is to obtain a working autoencoder, which we achieve using both a standard feed forward network and a convolutional network. We use data of dimension $1001 \times 128 \times 128$, and select our latent space to be of dimension $32 \time 32$. Figure 6.1 displays code for the network of Dense layers, while figure 6.2 displays code for the convolutional neural network. Figures 6.3 and 6.4 display the corresponding model summaries.
\begin{figure}
\centering
\includegraphics[scale=0.5]{ae1.png}
\caption{Dense Autoencoder Code}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{ae2.png}
\caption{Dense Autoencoder Summary}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{ae3.png}
\caption{Convolutional Autoencoder Code}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{ae4.png}
\caption{Convolutional Autoencoder Summary}
\end{figure}
We run the autoencoders on 10 random samples from data that was not touched during training, and visualize the latent space, along with the input and its autoencoder approximation. Those comparisons are seen in figure 6.5 for the dense encoder, and 6.6 for the convolutional encoder.
\begin{figure}
\centering
\includegraphics[scale=0.35]{ae5.png}
\caption{Dense Autoencoder Latent Space}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.35]{ae6.png}
\caption{Convolutional Autoencoder Latent Space}
\end{figure}
\par There is still much work remaining to integrate the autoencoder with the SINDy optimization procedure. The first concern is how to best integrate the SINDy procedure into the training. We will take as our first approach a training procedure that uses batches of 10. After the reconstruction loss is computed, we will perform the SINDy optimization on the latent space for the 10 samples, and combine that error with the autoencoder error. A key uncertainty here is how to best estimate time derivatives of the latent space variable, as well as how to compute its spatial derivatives.
\section{Conclusion}
\par We conclude this report by listing the key tasks moving forward.
\begin{enumerate}
    \item We need to perform a careful analysis of the derivation of the RCN for real fields. Much of the reasoning used for real fields is more delicate than for complex fields.
    \item We need to study the self-dual stationary solutions, and the role the Jacobian of the map from $(x,y)$ to $\bm{k}=(\theta_x,\theta_y)$ plays in the analysis.
    \item The autoencoder framework is completely uncoupled from the SINDy optimization procedure, and therefore we must merge them together.
    \item Discovering a model through SINDy like methods may not be the way to resolve this problem. However, I believe there are interesting machine learning experiments that can be performed to gain insight. For instance, autoencoders may be used to perform anomaly detection, which may be relevant if training an autoencoder on data prior to the formation of the stair-step pattern. We could also compare the performance of a pair of autoencoders on the elliptical solutions, where one autoencoder uses one latent variable, and the other uses two latent variables. Such a comparison may be used to suggest the emergence (or lack thereof) of another order parameter. Finally, reconstructing high resolution solutions of the microscopic model from coarse samples of the macroscopic model can be useful in determining the relevant order parameters.
\end{enumerate}









\end{document}
